version: '3.8'

services:
  llama-cpp-studio:
    build: .
    image: llama-cpp-studio:rocm
    ports:
      - "8080:8080"
      - "2000:2000"
    volumes:
      - ./data:/app/data
      - ./backend:/app/backend
      # Mount ROCm devices
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    environment:
      # Disable CUDA to use ROCm instead
      - CUDA_VISIBLE_DEVICES=""
      - RELOAD=true
      # ROCm environment variables
      - HSA_OVERRIDE_GFX_VERSION=10.3.0
      - HIP_VISIBLE_DEVICES=all
      - ROC_ENABLE_PRE_VEGA=1
      # Uncomment and set your HuggingFace API key
      # - HUGGINGFACE_API_KEY=your_huggingface_token_here
    devices:
      # AMD GPU access
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    # Enable privileged mode for GPU access
    privileged: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3
    cap_add:
      - SYS_ADMIN
    shm_size: '2gb'
