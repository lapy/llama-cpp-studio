feat: fine-tune memory estimation and improve GGUF metadata extraction

- Fine-tune RAM/VRAM estimation factors based on actual testing
  * Reduce KV cache factor from 0.35 to 0.30 (30% of theoretical)
  * Further reduce batch RAM overhead factors (0.08/0.04 for intermediate/QKV)
  * Reduce computation overhead from 512KB to 400KB per batch item
  * Update VRAM estimation to match RAM estimation factors
  * Achieve 87-95% accuracy across context sizes (4k-32k tokens)

- Improve GGUF metadata extraction for Qwen3 MoE models
  * Add fallback keys for embedding_length, attention_head_count, attention_head_count_kv
  * Support qwen3moe.*, qwen3.*, qwen.* metadata keys
  * Fix missing metadata extraction that caused fallback calculations

- Add hot reload support for Python development
  * Enable uvicorn --reload with RELOAD environment variable
  * Mount backend directory in all docker-compose files
  * Automatically reload on Python file changes for faster iteration

Testing:
- Verified estimation accuracy with Qwen3-30B model across multiple context sizes
- Tested actual RSS vs estimated RAM usage (87-95% accuracy)
- Confirmed KV cache and batch RAM scaling with context size

